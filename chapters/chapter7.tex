\chapter{Conclusions}\label{conclusions}
The success of Mu2e depends on many factors, 
one of which is the performance of the tracker. 
The tracker must provide an excellent momentum 
resolution, approximately 1 MeV/c FWHM, 
in order to distinguish the monochromatic CE 
signal from the background. 
The Mu2e collaboration has chosen a detector 
based on the straw tube technology.  
Its distinctive annular geometry is highly 
effective in reducing the background. 
The tracker is placed inside the 
DS, downstream from the ST, 
in a uniform 1 T magnetic field. The tracker  
has a modular design and consists of 18 tracking stations.

This Thesis provides a comprehensive 
contribution to the Mu2e tracker 
development, covering from its initial stages of 
commissioning to the optimization 
of DAQ and FEE and preliminary calibration 
steps. My work at Fermilab 
focused on thorough testing of the DAQ system 
from both hardware and 
software perspectives, and included 
involvement in the Vertical Slice Test 
(VST) of the tracker. 
The VST encompasses the entire testing 
chain, from the straws to the readout, 
to processed data on disk.
Throughout the chapters, we explored 
critical aspects of the tracker, focusing on the robustness and 
reliability of data acquisition 
processes and the crucial role of 
calibration in ensuring precise measurements. 
Part of my work was dedicated to the offline 
analysis, particularly to pre-pattern recognition 
studies, investigating techniques of   
flagging $\delta$-electrons during data taking. 
I performed the very first systematic study 
comparing the performance of two algorithms 
to determine the best approach for data-taking. 
\section{Commissioning of the tracker DAQ and FEE}
The commissioning of the tracker DAQ and FEE 
systems, described in Chapter 
\ref{commissioning}, has been a crucial step 
in validating the functionality 
and performance of the tracker readout chain.

The first test I performed was to verify the 
correct functioning of 
ROC buffering and to understand its logic. 
During these tests, a single 
ROC was connected to one DTC. Data were 
collected with digi-FPGAs pulsed 
by their internal pulsers, with the ROC set 
in external mode. The ROC's  
readout logic can be emulated with a bit-level 
C++ simulation, and I contributed to the simulation code development. 
The Monte Carlo simulation 
and the experimental data were compared in two different modes: 
\textit{underflow} and \textit{overflow} readout mode.

The ROC has an internal buffer which can store up to 255 hits, 
and, depending on \( T_{gen} \) and \( T_{EW} \), 
the total number of hits within the event window 
can be respectively less than or equal to 255. 
These configurations depend on whether or not it is getting filled up.
By studying the timing distributions of the signal delays 
between different channels, we 
were able to incorporate relative channel-to-channel delays into 
the simulation. The comparison 
between the data and the bit-level simulation 
demonstrated good agreement, 
validating the simulation accuracy in reproducing 
the expected ROC behavior in   
two modes. The agreement between 
the simulation and the experimental data was highly satisfactory,
with the deviations at a level of \( 10^{-3} \).

The second test focused on evaluating the 
performance of preamplifiers, 
particularly looking for dead channels, 
cross-talk between the channels, 
and unexpected pulse patterns. The first part 
of the test examined the channel occupancy, revealing 
dead channels and channels exhibiting a higher 
number of hits than expected. 
Cross-talk was observed in only the first channels, 
with an asymmetry in interference 
between odd and even channels. This phenomenon was 
attributed to the physical proximity 
of preamplifiers on the motherboard and the 
closeness of the first channels to each other. 
Although the cross-talk was not fully addressed at 
the time of writing, further investigations 
are ongoing. Subsequent waveform analysis helped explain 
abnormalities, particularly the inverted 
waveforms observed in some channels, which indicated 
the erroneous triggering of waveform 
on both the leading and trailing edges. The 
waveform analysis 
provided further insights into the charge and 
pulse height distributions, showing two or 
three peaks in their distribution. This was due 
to the ADCs and the test pulser having 
correlated clocks. 

Overall, these tests provided valuable results 
that will inform future improvements to the 
preamplifier design and its integration into 
the readout chain. They also laid the foundations 
for the development of real-time 
diagnostic tools, along with the identification of 
key performance issues.

In summary, this commissioning phase successfully validated 
the core functionalities 
of the tracker DAQ and FEE systems, ensuring they are ready 
for the next stages of 
testing and integration. The insights gained, particularly 
in handling ROC overflow 
and timing synchronization, will be essential for optimizing 
the system's performance 
during full detector operation. Further work will focus on 
expanding the scope of 
testing with more complex configurations and refining the 
DAQ software for future runs.


\section{First steps towards the station calibration}
Chapter \ref{planning} describes the initial steps towards 
the tracker calibration. Our primary objective is to perform a time calibration of 
the first assembled station using cosmic muons, with the aim of achieving 
a resolution on the longitudinal hit position better than 4 cm. 

This calibration requires determining the signal propagation velocity and 
channel-to-channel delays. 

When a charged particle passes through the straw tube gas volume, the 
resulting ionization charge generates an electric signal along the anode wire, 
which propagates to both ends of the straw and reaches the front-end 
electronics, where TDCs measure the signal arrival times, $t_1$ and $t_2$. 
The calibration will be performed by reconstructing the track position 
along the straw and correlating it with the arrival times. 

Due to operational constraints, such as the gas distribution, fragility, 
and space, a calibration with a station oriented vertically was considered
to be the best option. 
I focused on the reconstruction of $x_{\text{track}}$ using only the 
information about whether a straw was crossed or not by a cosmic muon. The only 
way to deduce $x_{\text{track}}$ from this "yes-no" information is to use the geometry of the station, 
which consists of a set of rotated panels arranged in four faces orthogonal 
to the detector axis. I examined potential 
biases and systematic errors that could arise from using the vertical orientation. 

The first step in my analysis involved studying the selection criteria for 
muons crossing a station and then various hit distribuitons 
resulting from the selection. 
The requirement of having at least one hit per face resulted in 
a non-uniform illumination, with almost no hits 
in the central region of the panel. 

To reconstruct the 3D trajectory of a cosmic muon
moving along the straight line, at least 
two 3D points along its track are needed. The $x$ and $y$ coordinates 
of each point can be determined by the intersection of one pair of 
non-parallel straws. This analysis indicates that the longitudinal bias along the straw 
range spans approximately [-6,6] cm. This indicates a significant systematic factor 
affecting the hit reconstruction. 
The 2D distribution of the longitudinal bias versus the 
true position shows four distinct spots along the longitudinal axis, each corresponding 
to overlap regions of different straws. 
The systematic shift in the reconstructed longitudinal position could be as large as $\pm$4 cm, 
with the magnitude varying along the wire. 
This suggests that the mean is not a reliable estimator of the bias under 
these conditions, making the calibration process more challenging.
The main issue with this configuration is that the stereo 
reconstruction biases for tracks with positive and negative values of $m_{yz}$
do not cancel out. 

In addition to the results, we must consider the rate and data-taking time. 
For the vertical orientation, the rate must be scaled by two additional 
factors compared to the horizontal one: one that accounts for the 
angular dependence of the flux, and the second one that considers the angle between the station and cosmics. 
However, estimating the rate is complex, as these factors are not the main contributors to the difference. 
With the horizontal orientation, it's possible to reconstruct a particle track with 
fewer requirements, relying on most muons being vertical. In contrast, with the 
vertical orientation, the analysis must include all biases for every straw, increasing 
the data volume needed as more parameters are involved.

Therefore, the horizontal orientation of the station is worth considering despite the difficulties
with connecting the gas supply lines.
A new mechanical solution to use the horizontal orientation is currently under development to address these issues.




\section{Pre-pattern recognition studies}

Given the high data volume expected during Mu2e data taking, 
\del{estimated at approximately}\add{which could be as large as} 7 PBytes per year,
optimizing memory usage and minimizing \add{the} CPU \add{time} consumption are critical.
One significant challenge is flagging $\delta$-electron hits, \del{since they}\add{which}
are the primary source 
of hits in the tracker, without compromising the efficiency of 
\del{CE hit detection and 
  track reconstruction}\add{reconstructing the CE tracks}.
\hlite{$\delta$-electrons originate from Compton 
scattered electrons, 
pair production electrons and positrons, and delta rays}.
\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{delta-electrons originate from delta rays - tautology? }

Compton 
scattered electrons 
are produced when photons from neutron capture interact with the 
detector material, and typically have energies of a few MeV.
Pair production electrons and positrons 
are generated during nuclear recoil processes, and delta rays are produced 
when high-energy charged particles collide with the detector material.

A detailed study of pre-pattern recognition and a comparison of \add{the} two 
\del{algorithms for }
$\delta$-electron flagging \add{algorithms} is \del{provided}\add{presented}
in Chapter \ref{delta}\del{ to address this issue}. 
This study has strong motivations. First, \del{Mu2e's 
primary goal}\add{the primary goal of Mu2e} is 
to detect the CE signal: flagging even a small fraction of hits generated 
by the CE as $\delta$-electrons \del{may reduce}\add{reduces} the CE track 
reconstruction efficiency. 
Second, \hlite{misidentifying muons and pions as protons or $\delta$-electrons
can result in inaccurate background estimates.}
\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{fix needed, quite desperately}

\hlite{Being able to identify 
protons and count the number of protons in one event is extremely 
important, since proton counting could complement the STM in determining 
the muon stopping rate. 
Simulations show that the majority (approximately 75\%) of hits in 
the tracker 
are generated by $\delta$-electrons, i.e., electrons and positrons 
with momenta below 20 MeV/c.}

\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{the sentences above are disconnected and not related to each other}


Two algorithms have been developed in Mu2e collaboration to identify 
\del{and exclude} $\delta$-electron\del{s} hits \del{from the reconstruction 
  process}: $FlagBkgHits$ and $DeltaFinder$.
\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{offline, flagged hits are excluded only from time clustering and pattern recognition,
but not from the track fit}

The $FlagBkgHits$ algorithm initially clusters hits on $x-y$ plane 
and on time and uses an ANN to classify them, 
while $DeltaFinder$ identifies clusters of hits consistent with those 
produced by 
low-momentum particles. Based on the mean energy deposited by a $seed$ 
in a station, hits with energy above 5 keV are considered 
proton hits, while $FlagBkgHits$ lacks the 
capability to identify proton hits.

\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{is it necessary to repeat such details in the summary? }

I conducted a systematic, $two-level$ comparison of the performance 
of the two algorithms:
\begin{enumerate}
    \item \textbf{hit-level comparison}: this study evaluates the accuracy 
    of individual hit flagging, providing a direct method to compare 
    the algorithms' performance. In terms of $\delta$-electron flagging, 
    the two algorithms have a similar performance. \hlite{However, 
    $DeltaFinder$ performs 
    better in flagging positrons due to the low statistics at energies 
    below 2 MeV/c.}
\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{this should be exactly the opposite - at very low energies, when the number
  of hits produced by a single particle is small, an ANN-based algorithm
  should do better. \\
  b) and a usual comment about statistics, which sounds like a science fiction
}

  This presents a disadvantage for $FlagBkgHits$ since 
  the ANN performance depends on statistics, and it struggles 
  to reconstruct hits 
  that register only a single hit per station. $FlagBkgHits$ flags 
  approximately 70\% more CE hits than $DeltaFinder$, as it analyzes 
  the $x-y$ plane and does not reconstruct hits in the $z$ coordinate.

  Additionally, $DeltaFinder$ 
  can \del{flag proton hits (approximately 84\%)}\add{identify protons and deuterons 
    with the corresponding hit flagging efficiency of about 84\%}, a task that $FlagBkgHits$ 
  cannot perform. 

  \hlite{It is crucial that hits corresponding to $p\bar{p}$ annihilation 
    are not flagged.}
  \todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
  {a) I already read this claim a few sentences earlier, why repeat? \\
    b) no ppbar annihilation, search-and-replace globally
  }

  $FlagBkgHits$ has been trained on datasets lacking 
  muon and pion hits and thus flags these particles at rates roughly 
  4 and 3.3 times higher, respectively, than $DeltaFinder$;

\item  \textbf{high-level comparison}: this study 
assesses the algorithms' impact on 
\del{subsequent event reconstruction stages, 
particularly how each contributes to accurate}\add{the} 
track reconstruction \add{efficiency}. The main difference 
between the two algorithms is in the number 
of reconstructed tracks using CE data samples, 
which is 16\% higher for $FlagBkgHits$. 
This increase is due to $FlagBkgHits$ \del{failing to 
flag protons properly, allowing these 
particles to be sent to the reconstruction stage}
\add{leaving in enough proton hits for the pattern
  recognition to find the corresponding tracks}. 
\hlite{The difference in the reconstruction of $p\bar{p}$ 
events, measured as the fraction of events with at 
least two reconstructed tracks, is approximately 
22\% higher with $DeltaFinder$.}
\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{a difference cant be higher, a difference is not a fraction.. .rephrase}

\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{need to state that the CE reco efficiencies are within 1\% relative from each other}

\end{enumerate}

\hlite{An important aspect of the study was the 
timing performance of the two algorithms.
For $FlagBkgHits$, it was necessary to 
account for the time spent creating the $StereoHit$s, 
while $DeltaFinder$ independently reconstructed 
the $seed$s. The timing performance was 
studied across all data samples, and showed that 
the difference in processing time between 
the two algorithms is negligible (with 
$FlagBkgHits$ having an advantage of less than 1 ms), 
considering that the entire reconstruction 
process takes few ms per event.}
\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{just give a quantified summary, procedural details are not needed}

The primary drawback of $FlagBkgHits$ is its 
reliance on supervised training using 
CE and $\delta$-electron samples. This 
approach fails when other particles, such 
as cosmic muons and those from $p\bar{p}$ 
annihilation, are introduced into the algorithm.

\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{in principle, one could add other sample to training,
the issue is that it is becoming more and more cumbersome technically}

Furthermore, the training was performed using 
Monte Carlo data rather than real data, 
posing a potential risk when transitioning 
to actual data taking.
\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{it is not a risk, but rather what do you do in early data taking?
what is the strategy of training with the data ?}

\del{Additionally, as an ANN-based method, $FlagBkgHits$ performs 
poorly when statistics are very low and lacks 
a well-defined method for proton flagging.}
\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{avoid using words like 'poorly', especially in writing}


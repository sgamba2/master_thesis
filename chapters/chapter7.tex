\chapter{Conclusions}\label{conclusions}
The success of Mu2e depends on many factors, 
one of which is the performance of the tracker. 
The tracker must provide an excellent momentum 
resolution, approximately 1 MeV/c FWHM, 
in order to distinguish the monochromatic CE 
signal from the background. 
The Mu2e collaboration has chosen a detector 
based on the straw tube technology.  
Its distinctive annular geometry is highly 
effective in reducing the background. 
The tracker is placed inside the 
DS, downstream from the ST, 
in a uniform 1 T magnetic field. The tracker  
has a modular design and consists of 18 tracking stations.

This Thesis provides a comprehensive 
contribution to the Mu2e tracker 
development, covering from its initial stages of 
commissioning to the optimization 
of DAQ and FEE and preliminary calibration 
steps. My work at Fermilab 
focused on thorough testing of the DAQ system 
from both hardware and 
software perspectives, and included 
involvement in the Vertical Slice Test 
(VST) of the tracker. 
The VST encompasses the entire testing 
chain, from the straws to the readout, 
to processed data on disk.
Throughout the chapters, we explored 
critical aspects of the tracker \add{performance},
focusing on the robustness and 
reliability of data acquisition 
processes and the crucial role of 
calibration in ensuring precise measurements. 
Part of my work was dedicated to the offline 
analysis, particularly to pre-pattern recognition 
studies, investigating techniques of   
flagging $\delta$-electrons during data taking. 
I performed the very first systematic study 
comparing the performance of two algorithms 
to determine the best approach for data-taking. 
\section{Commissioning of the tracker DAQ and FEE}
The commissioning of the tracker DAQ and FEE 
systems, described in Chapter 
\ref{commissioning}, has been a crucial step 
in validating the functionality 
and performance of the tracker readout chain.

The first test I performed was to verify the 
correct functioning of 
\add{the} ROC buffering and to understand its logic. 
During these tests, a single 
ROC was connected to one DTC. Data were 
collected with digi-FPGAs pulsed 
by their internal pulsers, with the ROC set 
in external mode. The ROC's  
readout logic can be emulated with a bit-level 
C++ simulation, and I contributed to the simulation code development. 
The Monte Carlo simulation 
and the experimental data were compared in two different modes: 
\textit{underflow} and \textit{overflow} readout mode.

The ROC has an internal buffer which can store up to 255 hits, 
and, depending on \( T_{gen} \) and \( T_{EW} \), 
the total number of hits within the event window 
can be respectively less than or equal to 255. 
These configurations depend on whether or not \del{it}\add{the buffer}
is getting filled up.
By studying the timing distributions of the signal delays 
between different channels, we 
were able to incorporate relative channel-to-channel delays into 
the simulation. The comparison 
between the data and the bit-level simulation 
demonstrated good agreement, 
validating the simulation accuracy in reproducing 
the expected ROC behavior in   
two modes. The agreement between 
the simulation and the experimental data was highly satisfactory,
with the deviations at a level of \( 10^{-3} \).

The second test focused on evaluating the 
performance of preamplifiers, 
particularly looking for dead channels, 
cross-talk between the channels, 
and unexpected pulse patterns. The first part 
of the test examined the channel occupancy, revealing 
dead channels and channels exhibiting a higher 
number of hits than expected. 
Cross-talk was observed in only the first channels, 
with an asymmetry in interference 
between odd and even channels. This phenomenon was 
attributed to the physical proximity 
of preamplifiers on the motherboard and the 
closeness of the first channels to each other. 
Although the cross-talk was not fully addressed at 
the time of writing, further investigations 
are ongoing. Subsequent waveform analysis helped explain 
abnormalities, particularly the inverted 
waveforms observed in some channels, which indicated 
the erroneous triggering of waveform 
on both the leading and trailing edges. The 
waveform analysis 
provided further insights into the charge and 
pulse height distributions, showing two or 
three peaks in their distribution. This was due 
to the ADCs and the test pulser having 
correlated clocks. 

Overall, these tests provided valuable results 
that will inform future improvements to the 
preamplifier design and its integration into 
the readout chain. They also laid \del{the foundations}
\add{a foundation} 
for the development of real-time 
diagnostic tools, along with the identification of 
key performance issues.

In summary, this commissioning phase successfully validated 
the core functionalities 
of the tracker DAQ and FEE systems, ensuring they are ready 
for the next stages of 
testing and integration. The insights gained, particularly 
in handling ROC overflow 
and timing synchronization, will be essential for optimizing 
the system's performance 
during full detector operation. Further work will focus on 
expanding the scope of 
testing with more complex configurations and refining the 
DAQ software for future runs.


\section{First steps towards the station calibration}
Chapter \ref{planning} describes the initial steps towards 
the tracker calibration. Our primary objective is to perform a time calibration of 
the first assembled station using cosmic muons, with the aim of achieving 
a resolution on the longitudinal hit position better than 4 cm. 

This calibration requires determining the signal propagation velocity and 
channel-to-channel delays. 

When a charged particle passes through the straw tube gas volume, the 
resulting ionization charge generates an electric signal along the anode wire, 
which propagates to both ends of the straw and reaches the front-end 
electronics, where TDCs measure the signal arrival times, $t_1$ and $t_2$. 
The calibration will be performed by reconstructing the track position 
along the straw and correlating it with the arrival times. 

Due to operational constraints, such as the gas distribution, fragility, 
and space, a calibration with a station oriented vertically was considered
to be the best option. 
I focused on the reconstruction of $x_{\text{track}}$ using only the 
information about whether a straw was crossed or not by a cosmic muon. The only 
way to deduce $x_{\text{track}}$ from this "yes-no" information is to use the geometry of the station, 
which consists of a set of rotated panels arranged in four faces orthogonal 
to the detector axis. I examined potential 
biases and systematic errors that could arise from using the vertical orientation. 

The first step in my analysis involved studying the selection criteria for 
muons crossing a station and then various hit distribuitons 
resulting from the selection. 
The requirement of having at least one hit per face resulted in 
a non-uniform illumination, with almost no hits 
in the central region of the panel. 

To reconstruct the 3D trajectory of a cosmic muon
moving along the straight line, at least 
two 3D points along its track are needed. The $x$ and $y$ coordinates 
of each point can be determined by the intersection of one pair of 
non-parallel straws. This analysis indicates that the longitudinal bias along the straw 
range spans approximately [-6,6] cm. This indicates a significant systematic factor 
affecting the hit reconstruction. 
The 2D distribution of the longitudinal bias versus the 
true position shows four distinct spots along the longitudinal axis, each corresponding 
to overlap regions of different straws. 
The systematic shift in the reconstructed longitudinal position could be as large as $\pm$4 cm, 
with the magnitude varying along the wire. 
This suggests that the mean is not a reliable estimator of the bias under 
these conditions, making the calibration process more challenging.
The main issue with this configuration is that the stereo 
reconstruction biases for tracks with positive and negative values of $m_{yz}$
do not cancel out. 

In addition to the results, we must consider the rate and \add{the} data-taking time. 
For the vertical orientation, the rate must be scaled by two additional 
factors compared to the horizontal one: one that accounts for the 
angular dependence of the flux, and the second one that considers the angle between the station and cosmics. 
However, estimating the rate is complex, as these factors are not the main contributors to the difference. 
With the horizontal orientation, it's possible to reconstruct a particle track with 
fewer requirements, relying on most muons being vertical. In contrast, with the 
vertical orientation, the analysis must include all biases for every straw, increasing 
the data volume needed as more parameters are involved.

Therefore, the horizontal orientation of the station is worth considering despite the difficulties
with connecting the gas supply lines.
A new mechanical solution to use the horizontal orientation is currently under development to address these issues.




\section{Pre-pattern recognition studies}

Given the high data volume expected during Mu2e data taking, 
which could be as large as 7 PBytes per year,
optimizing memory usage and minimizing the CPU time consumption are critical.
One significant challenge is flagging $\delta$-electron hits (i.e. electrons and positrons 
with momenta below 20 MeV/c), which are the primary source 
of hits in the tracker, without compromising the efficiency of 
reconstructing the CE tracks. These particles come from Compton 
scattering, electron-positron pairs, 
and delta rays. Compton scattered electrons 
are produced when photons from neutron capture interact with the 
detector material, and typically have energies of a few MeV.
Pair production electrons and positrons 
are generated during \hlite{nuclear recoil processes},
\todo[inline,color=green!20,linecolor=gray,tickmarkheight=10pt]
{what is this ?}

and delta rays are produced 
when high-energy charged particles collide with the detector material.

A detailed study of pre-pattern recognition and a comparison of the two 
$\delta$-electron flagging algorithms is presented 
in Chapter \ref{delta}. This study has strong motivations. First, the primary goal of Mu2e is 
to detect the CE signal: flagging even a small fraction of hits generated 
by the CE as $\delta$-electrons reduces the CE track 
reconstruction efficiency. Second, misidentification of muons 
and pions as $\delta$-electrons may result in errouneous background estimate. 
Another reason is that counting the number of protons will be a complementary procedure to the 
STM to determine the muon stopping rate.


Two algorithms have been developed in Mu2e collaboration to identify 
$\delta$-electron hits: $FlagBkgHits$ and $DeltaFinder$.
The $FlagBkgHits$ algorithm initially clusters the hits and then uses an ANN 
to classify them, while $DeltaFinder$ reconstructs only clusters of hits  
consistent with those produced by low-momentum particles or protons and deuterons.


I conducted a systematic, $two-level$ comparison of the performance 
of the two algorithms:
\begin{enumerate}
    \item \textbf{hit-level comparison}: this study evaluates the accuracy 
    of individual hit flagging, providing a direct method to compare 
    the algorithms' performance. In terms of $\delta$-electron flagging, 
    the two algorithms have a similar performance. 
    At low momentum values (below 1-2 MeV/c), positron hit flagging 
    is more efficient because positrons are not produced by Compton scattering. 
    Electrons often produce only one or two hits, making them harder 
    to flag, which reduces electron flagging 
    efficiency. The $FlagBkgHits$ algorithm works better for electrons below 
    2 MeV, since the minimum number of hits to form a $seed$ for 
    $DeltaFinder$ is three. At higher momentum, both positron and 
    electron flagging efficiency become similar, but overall efficiency 
    decreases as momentum increases due to a wider spread of hits.
    $FlagBkgHits$ flags approximately 70\% more CE hits than $DeltaFinder$.
    The main difference between the two algorithms results from  
    the primitives: $StereoHit$s and $seed$s. When a $\delta$-electron hits three 
    straws, the $StereoHit$ creates multiple intersection points, while the 
    intersection of hit wires and the consequent reconstruction of 
    a $seed$ allows to better determine the delta electron segment position in 3D. 
    Furthermore, $DeltaFinder$ can flag proton hits (approximately 84\%), while $FlagBkgHits$ 
    can only flag hits based on large energy deposits, but it does not specifically identify proton candidates.
  $FlagBkgHits$ has been trained on datasets lacking 
  muon and pion hits and thus flags these particles at rates roughly 
  4 and 3.3 times higher, respectively, than $DeltaFinder$;

\item  \textbf{high-level comparison}: this study 
assesses the algorithms' impact on 
the track reconstruction efficiency. The main observed difference between the two algorithms 
is an excess of tracks in the right part of the 
$FlagBkgHits$ momentum distribution of reconstructed tracks. 
This increase is due to $FlagBkgHits$ leaving in 
enough proton hits for the pattern 
recognition to find the corresponding tracks. 
For both 1BB and 2BB datasets, the relative difference between the 
CE reconstruction efficiencies corresponding to two algorithms is less than 1\%.
Moreover, the $DeltaFinder$ algorithm shows an advantage of 
approximately 22\% in reconstructing $\bar{p}$ two tracks.

\end{enumerate}

An important aspect of the study was the 
timing performance of the two algorithms. The Mu2e specification states 
that the processing time should not exceed 5 ms/event. The 
difference in processing time is approximately 0.13 ms/event with 
1BB pileup and 0.39 ms/event with 2BB pileup, with an advantage for $FlagBkgHits$. 
However, this difference does not appear to be critical.

The primary drawback of $FlagBkgHits$ is its 
reliance on supervised training using 
CE and $\delta$-electron samples. This 
approach fails when other particles, such 
as cosmic muons and those from $p\bar{p}$ 
annihilation, are introduced into the algorithm.
Additional samples could be added to the training 
process, but these would increase the technical complexity. 

Furthermore, the training was performed using \add{the} 
Monte Carlo data rather than \add{the} real data, 
posing a potential risk when transitioning 
to \del{actual}\add{the} data taking. The focus should be on how 
to handle training during early data taking and 
determining the strategy for incorporating real data into the training process.

